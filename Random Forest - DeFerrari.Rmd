---
title: "Random Forest Model Fitting and Diagnostics - DeFerrari"
output: html_notebook
---

### Library Read in
```{r}
library(tidyr)
library(GGally)
library(tidyverse)
library(dplyr)
library(randomForest)
library(caret)
library(combinat)
```


### Data Read in and Pre-Processing
```{r}
Red_wine<- read.csv("winequality-red.csv")
White_wine<- read.csv("winequality-white.csv")
```

```{r}
# Reformatting the red wine data
Red.wine <- separate(Red_wine, col = "fixed.acidity.volatile.acidity.citric.acid.residual.sugar.chlorides.free.sulfur.dioxide.total.sulfur.dioxide.density.pH.sulphates.alcohol.quality",
                           into = c("fixed.acidity", "volatile.acidity", "citric.acid", "residual.sugar", "chlorides", "free.sulfur.dioxide", "total.sulfur.dioxide", "density", "pH", "sulphates", "alcohol", "quality"),
                           sep = ";")

# Reformatting the white wine data
White.wine <- separate(White_wine, col = "fixed.acidity.volatile.acidity.citric.acid.residual.sugar.chlorides.free.sulfur.dioxide.total.sulfur.dioxide.density.pH.sulphates.alcohol.quality",
                           into = c("fixed.acidity", "volatile.acidity", "citric.acid", "residual.sugar", "chlorides", "free.sulfur.dioxide", "total.sulfur.dioxide", "density", "pH", "sulphates", "alcohol", "quality"),
                           sep = ";")

# Add in our color wine columns for Red and White
Red.wine <- Red.wine %>%
  mutate(Color = "Red")
White.wine<- White.wine %>%
  mutate(Color = "White")

# Join the data
Wine_joined<- full_join(White.wine, Red.wine, by = intersect(names(White.wine), names(Red.wine)))

# Convert all columns but color to numeric
Wine_joined <- Wine_joined %>%
  mutate_at(vars(-Color), as.numeric)

# Create our categorical response variable "quality" for our classification model
Wine_joined <- Wine_joined %>%
  mutate(quality = case_when(
    quality %in% 3:4 ~ "Poor",
    quality %in% 5:6 ~ "Average",
    quality == 7 ~ "Good",
    quality %in% 8:9 ~ "Great"
  ))
Wine_joined$quality<- factor(Wine_joined$quality, levels = c("Poor", "Average", "Good", "Great"))

# Set our seed our split the data into training and testing splits
set.seed(123)
Wine_training_index<- sample(1:nrow(Wine_joined), 0.6*nrow(Wine_joined))
Wine_training<- Wine_joined[Wine_training_index, ]
Wine_testing<- Wine_joined[-Wine_training_index, ]
```


### Model Fittings - Initial
```{r}
rf_wine = randomForest(quality ~ ., data = Wine_training, mtry = 5,
importance = TRUE)

rf_wine
```

```{r}
importance(rf_wine)
```

```{r}
varImpPlot(rf_wine)
```
We can see that color had little affect on the quality predictions while alcohol content had much more impact.
Now let us see how the model performs on our test dataset
```{r}
Yhat <- predict(rf_wine, newdata = Wine_testing)
table(Yhat, Wine_testing$quality)
```
```{r}
mean(Yhat != Wine_testing$quality) 
```

### Model Tuning - Just Mtry
```{r}
# establish the train control
train_control <- trainControl(method = "cv", number = 5)

# Grid of mtry values
tune_grid <- expand.grid(mtry = seq(1, 10, by = 1))

rf_wine_cv <- train(
  quality ~ .,
  data = Wine_training,
  method = "rf",
  trControl = train_control,
  tuneGrid = tune_grid,
  ntree = 500 
)
```


```{r}
rf_wine_cv
```
```{r}
predictions <- predict(rf_wine_cv, newdata = Wine_testing)
confusion_matrix <- confusionMatrix(predictions, Wine_testing$quality)
print(confusion_matrix)
```
### Model Tuning - Mtry and Ntree
```{r}
# Set up training control
train_control <- trainControl(method = "cv", number = 5)

# Define the grid for mtry
mtry_grid <- expand.grid(mtry = seq(1, 6, by = 1))

# Function to train and evaluate Random Forest with a specific ntree
train_rf_with_ntree <- function(ntree_value) {
  set.seed(123)  # For reproducibility
  rf_model <- train(
    quality ~ ., 
    data = Wine_training,
    method = "rf",
    trControl = train_control,
    tuneGrid = mtry_grid,
    ntree = ntree_value
  )
  return(rf_model)
}

# Define a range of ntree values to try
ntree_values <- seq(100, 1000, by = 100)

# Train and store results for each ntree value
results <- list()
for (ntree in ntree_values) {
  results[[as.character(ntree)]] <- train_rf_with_ntree(ntree)
}

# Extract the best model based on accuracy
best_model <- NULL
best_accuracy <- 0

for (ntree in names(results)) {
  model <- results[[ntree]]
  accuracy <- max(model$results$Accuracy)
  if (accuracy > best_accuracy) {
    best_accuracy <- accuracy
    best_model <- model
  }
}

# Print the best model and its parameters
print(best_model)
```
```{r}
best_model
```
```{r}
# Evaluate the best model on the test set
predictions <- predict(best_model, newdata = Wine_testing)
confusion_matrix <- confusionMatrix(predictions, Wine_testing$quality)
print(confusion_matrix)
```

```{r}
# This will show us the progress of our results as we move from 100 to 1000 n trees
results
```
As we can see - ntrees did not change the variable that much so we are still using mtry = 2 with the default ntree = 500



